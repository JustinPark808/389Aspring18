{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass Classification with Keras\n",
    "\n",
    "**Course**: CMSC 389A Practical Deep Learning  \n",
    "**Author**: Sujith Vishwajith\n",
    "\n",
    "**Task**: Up until now we have mainly been talking about binary classification. That is identifying whether an example either belongs to the class (1) or doesn't (0). We will now discuss multiclass classification which allows you to decide which class an example belongs to out of more than 2 options. In our example today, we will be classifying which type of Iris a flower is based on its features. There are three classes of Iris flowers: Iris Setosa, Iris Versicolor, and Iris Virginica. Our features for the plant are:\n",
    "1. Sepal Length (cm)\n",
    "2. Sepal Width (cm)\n",
    "3. Petal Length (cm)\n",
    "4. Petal Width (cm)\n",
    "\n",
    "This notebook is meant to supplement this week's lecture and give a reference for Practical 2.\n",
    "\n",
    "**Packages**  \n",
    "Lets import the following required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Fixed seed for reproducibility\n",
    "kSEED = 5\n",
    "np.random.seed(kSEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "We'll be using the Iris dataset provided by UCI [here](http://archive.ics.uci.edu/ml/machine-learning-databases/iris/) to train our model.\n",
    "\n",
    "**Reading the Data** \n",
    "Let's first load and read the dataset into our program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'Iris.csv'\n",
    "columns = [\n",
    "    'Sepal Length (cm)',\n",
    "    'Sepal Width (cm)',\n",
    "    'Petal Length (cm)',\n",
    "    'Petal Width (cm)',\n",
    "    'Species'\n",
    "]\n",
    "\n",
    "df = pd.read_csv(file_name, names=columns, delimiter=',', header=0)\n",
    "df[columns[:-1]] = df[columns[:-1]].astype(float)\n",
    "df[columns[-1]] = df[columns[-1]].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a peek at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sepal Length (cm)</th>\n",
       "      <th>Sepal Width (cm)</th>\n",
       "      <th>Petal Length (cm)</th>\n",
       "      <th>Petal Width (cm)</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sepal Length (cm)  Sepal Width (cm)  Petal Length (cm)  Petal Width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "1                4.9               3.0                1.4               0.2   \n",
       "2                4.7               3.2                1.3               0.2   \n",
       "3                4.6               3.1                1.5               0.2   \n",
       "4                5.0               3.6                1.4               0.2   \n",
       "\n",
       "       Species  \n",
       "0  Iris-setosa  \n",
       "1  Iris-setosa  \n",
       "2  Iris-setosa  \n",
       "3  Iris-setosa  \n",
       "4  Iris-setosa  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, let's normalize all the values between -1 and 1. Remember that this only applied to continous values not one hot class encodings (e.g. Male -> 0, Female -> 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize all the values between -1 and 1 as discussed in class\n",
    "for feature in df.columns[:-1]:\n",
    "    max_value = df[feature].max()\n",
    "    min_value = df[feature].min()\n",
    "    mean_value = df[feature].mean()\n",
    "    df[feature] = (df[feature] - mean_value) / (max_value - min_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now examine the data after normalizing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sepal Length (cm)</th>\n",
       "      <th>Sepal Width (cm)</th>\n",
       "      <th>Petal Length (cm)</th>\n",
       "      <th>Petal Width (cm)</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.206481</td>\n",
       "      <td>0.185833</td>\n",
       "      <td>-0.399774</td>\n",
       "      <td>-0.416111</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.262037</td>\n",
       "      <td>-0.022500</td>\n",
       "      <td>-0.399774</td>\n",
       "      <td>-0.416111</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.317593</td>\n",
       "      <td>0.060833</td>\n",
       "      <td>-0.416723</td>\n",
       "      <td>-0.416111</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.345370</td>\n",
       "      <td>0.019167</td>\n",
       "      <td>-0.382825</td>\n",
       "      <td>-0.416111</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.234259</td>\n",
       "      <td>0.227500</td>\n",
       "      <td>-0.399774</td>\n",
       "      <td>-0.416111</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sepal Length (cm)  Sepal Width (cm)  Petal Length (cm)  Petal Width (cm)  \\\n",
       "0          -0.206481          0.185833          -0.399774         -0.416111   \n",
       "1          -0.262037         -0.022500          -0.399774         -0.416111   \n",
       "2          -0.317593          0.060833          -0.416723         -0.416111   \n",
       "3          -0.345370          0.019167          -0.382825         -0.416111   \n",
       "4          -0.234259          0.227500          -0.399774         -0.416111   \n",
       "\n",
       "       Species  \n",
       "0  Iris-setosa  \n",
       "1  Iris-setosa  \n",
       "2  Iris-setosa  \n",
       "3  Iris-setosa  \n",
       "4  Iris-setosa  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the examples into a data variable\n",
    "\n",
    "Similar to what we did before, we will create a list `data` that contains all our examples in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Example:\n",
    "    \"\"\"\n",
    "    Class to represent a data example.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, features, label):\n",
    "        \"\"\"\n",
    "        Create a new example.\n",
    "\n",
    "        :param label: The label (0 / 1) of the example\n",
    "        :param vocab: The real valued features of patient (list)\n",
    "        \"\"\"\n",
    "        self.features = features\n",
    "        self.label = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for row in df.itertuples():\n",
    "    label = row[-1]\n",
    "    features = row[1:-1]\n",
    "    example = Example(features, label)\n",
    "    data.append(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = shuffle(data, random_state=kSEED)\n",
    "\n",
    "X = [example.features for example in data]\n",
    "y = [example.label for example in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding the Labels\n",
    "\n",
    "Before, our output variable only contained a binary label (either a 1 or a 0) that we wanted to predict. In this case, we have 3 potential output classes represented as strings. A common practice to use in this scenario is to create a one hot encoding for each class. A one-hot encoding is a matrix where each class is assigned an index and if the value at the index is True (>1), the data point belongs to that class. For example lets say our encoding looks like the following:\n",
    "\n",
    "Iris Setosa (index 0), Iris Versicolor (index 1), Iris Virginica (index 2)  \n",
    "If flower is Iris Setosa: `[1,0,0]`  \n",
    "If flower is Iris Versicolor: `[0,1,0]`  \n",
    "If flower is Iris Virginica: `[0,0,1]`  \n",
    "\n",
    "This is nice because our network can simply output a 1x3 vector (3 output nodes) where it contains the probability of being that class at each index. We can then just take the class with the highest probability and that is our class.\n",
    "\n",
    "Encoding the classes is really easy to do if you take advantage of sklearn's `Label Encoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder =  LabelEncoder()\n",
    "encoded_y = encoder.fit_transform(y)\n",
    "dummy_y = pd.get_dummies(encoded_y).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at the encoded values for the fist 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris-versicolor -> [0 1 0]\n",
      "Iris-virginica -> [0 0 1]\n",
      "Iris-virginica -> [0 0 1]\n",
      "Iris-setosa -> [1 0 0]\n",
      "Iris-virginica -> [0 0 1]\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print('{:} -> {:}'.format(y[i], dummy_y[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test Split\n",
    "\n",
    "We will take a random 80% for training examples and a random 20% for testing. We can do this quickly using sklearn's train test split function.\n",
    " \n",
    "Note how we use `dummy_y` instead of `y` as it contains our encoded vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Examples: 150\n",
      "Train Examples: 120\n",
      "Test Examples:   30\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,dummy_y,test_size=0.2, random_state=kSEED)\n",
    "\n",
    "print('Total Examples: {:}\\nTrain Examples: {:}\\nTest Examples: {:4d}'.format(len(data), len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the Model\n",
    "\n",
    "Notice how our model architecture looks extremely similar to the one we used for the binary classification task on the Diabetes dataset. The only big difference here is in the last line of the model where our final dense layer contains 3 neurons rather than 1. This is because each neuron will correspond to each of the Iris classes and contain a probability for each class.\n",
    "\n",
    "For example if the model output was `[0.8, 0.1, 0.1]`, then the predicted class is Iris Setosa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(16,input_shape=(4,),activation='relu'))\n",
    "model.add(Dense(8,activation='relu'))\n",
    "model.add(Dense(3,activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the Model\n",
    "\n",
    "Before we used the `binary_crossentropy` loss in the Diabetes dataset. We will now use the `category_crossentropy` loss to handle more than two classes. Check out the link [here](http://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#cross-entropy) for more information on the differences between the two but just remember that binary is for two classes and categorical is for more than two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Summary\n",
    "\n",
    "To get a quick overview of our model, we can use the `.summary()` property of Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 243\n",
      "Trainable params: 243\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "\n",
    "We will train then neural network on the data we setup before. We will train for 100 epochs with a batch size of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "120/120 [==============================] - 0s - loss: 2.6940 - acc: 0.3250     \n",
      "Epoch 2/100\n",
      "120/120 [==============================] - 0s - loss: 2.1365 - acc: 0.3250     \n",
      "Epoch 3/100\n",
      "120/120 [==============================] - 0s - loss: 1.6771 - acc: 0.3250     \n",
      "Epoch 4/100\n",
      "120/120 [==============================] - 0s - loss: 1.3775 - acc: 0.3250     \n",
      "Epoch 5/100\n",
      "120/120 [==============================] - 0s - loss: 1.2051 - acc: 0.3417     \n",
      "Epoch 6/100\n",
      "120/120 [==============================] - 0s - loss: 1.1052 - acc: 0.4333     \n",
      "Epoch 7/100\n",
      "120/120 [==============================] - 0s - loss: 1.0414 - acc: 0.4083     \n",
      "Epoch 8/100\n",
      "120/120 [==============================] - 0s - loss: 1.0015 - acc: 0.3667     \n",
      "Epoch 9/100\n",
      "120/120 [==============================] - 0s - loss: 0.9695 - acc: 0.3500     \n",
      "Epoch 10/100\n",
      "120/120 [==============================] - 0s - loss: 0.9445 - acc: 0.4250     \n",
      "Epoch 11/100\n",
      "120/120 [==============================] - 0s - loss: 0.9191 - acc: 0.6750     \n",
      "Epoch 12/100\n",
      "120/120 [==============================] - 0s - loss: 0.8976 - acc: 0.7333     \n",
      "Epoch 13/100\n",
      "120/120 [==============================] - 0s - loss: 0.8733 - acc: 0.8000     \n",
      "Epoch 14/100\n",
      "120/120 [==============================] - 0s - loss: 0.8495 - acc: 0.7583     \n",
      "Epoch 15/100\n",
      "120/120 [==============================] - 0s - loss: 0.8270 - acc: 0.7750     \n",
      "Epoch 16/100\n",
      "120/120 [==============================] - 0s - loss: 0.8031 - acc: 0.7333     \n",
      "Epoch 17/100\n",
      "120/120 [==============================] - 0s - loss: 0.7793 - acc: 0.7917     \n",
      "Epoch 18/100\n",
      "120/120 [==============================] - 0s - loss: 0.7562 - acc: 0.7917     \n",
      "Epoch 19/100\n",
      "120/120 [==============================] - 0s - loss: 0.7335 - acc: 0.7750     \n",
      "Epoch 20/100\n",
      "120/120 [==============================] - 0s - loss: 0.7139 - acc: 0.7917     \n",
      "Epoch 21/100\n",
      "120/120 [==============================] - 0s - loss: 0.6942 - acc: 0.8083     \n",
      "Epoch 22/100\n",
      "120/120 [==============================] - 0s - loss: 0.6777 - acc: 0.8000     \n",
      "Epoch 23/100\n",
      "120/120 [==============================] - 0s - loss: 0.6601 - acc: 0.7333     \n",
      "Epoch 24/100\n",
      "120/120 [==============================] - 0s - loss: 0.6417 - acc: 0.7833     \n",
      "Epoch 25/100\n",
      "120/120 [==============================] - 0s - loss: 0.6285 - acc: 0.8250     \n",
      "Epoch 26/100\n",
      "120/120 [==============================] - 0s - loss: 0.6124 - acc: 0.8250     \n",
      "Epoch 27/100\n",
      "120/120 [==============================] - 0s - loss: 0.6001 - acc: 0.7917     \n",
      "Epoch 28/100\n",
      "120/120 [==============================] - 0s - loss: 0.5838 - acc: 0.8000     \n",
      "Epoch 29/100\n",
      "120/120 [==============================] - 0s - loss: 0.5697 - acc: 0.7917     \n",
      "Epoch 30/100\n",
      "120/120 [==============================] - 0s - loss: 0.5585 - acc: 0.8250     \n",
      "Epoch 31/100\n",
      "120/120 [==============================] - 0s - loss: 0.5490 - acc: 0.8083     \n",
      "Epoch 32/100\n",
      "120/120 [==============================] - 0s - loss: 0.5336 - acc: 0.8500     \n",
      "Epoch 33/100\n",
      "120/120 [==============================] - 0s - loss: 0.5263 - acc: 0.8333     \n",
      "Epoch 34/100\n",
      "120/120 [==============================] - 0s - loss: 0.5133 - acc: 0.8333     \n",
      "Epoch 35/100\n",
      "120/120 [==============================] - 0s - loss: 0.5044 - acc: 0.8333     \n",
      "Epoch 36/100\n",
      "120/120 [==============================] - 0s - loss: 0.4990 - acc: 0.8083     \n",
      "Epoch 37/100\n",
      "120/120 [==============================] - 0s - loss: 0.4992 - acc: 0.8500     \n",
      "Epoch 38/100\n",
      "120/120 [==============================] - 0s - loss: 0.4780 - acc: 0.8583     \n",
      "Epoch 39/100\n",
      "120/120 [==============================] - 0s - loss: 0.4710 - acc: 0.8417     \n",
      "Epoch 40/100\n",
      "120/120 [==============================] - 0s - loss: 0.4650 - acc: 0.8500     \n",
      "Epoch 41/100\n",
      "120/120 [==============================] - 0s - loss: 0.4577 - acc: 0.8500     \n",
      "Epoch 42/100\n",
      "120/120 [==============================] - 0s - loss: 0.4524 - acc: 0.8417     \n",
      "Epoch 43/100\n",
      "120/120 [==============================] - 0s - loss: 0.4448 - acc: 0.8583     \n",
      "Epoch 44/100\n",
      "120/120 [==============================] - 0s - loss: 0.4382 - acc: 0.8667     \n",
      "Epoch 45/100\n",
      "120/120 [==============================] - 0s - loss: 0.4335 - acc: 0.8583     \n",
      "Epoch 46/100\n",
      "120/120 [==============================] - 0s - loss: 0.4263 - acc: 0.8833     \n",
      "Epoch 47/100\n",
      "120/120 [==============================] - 0s - loss: 0.4202 - acc: 0.8833     \n",
      "Epoch 48/100\n",
      "120/120 [==============================] - 0s - loss: 0.4187 - acc: 0.8500     \n",
      "Epoch 49/100\n",
      "120/120 [==============================] - 0s - loss: 0.4088 - acc: 0.8750     \n",
      "Epoch 50/100\n",
      "120/120 [==============================] - 0s - loss: 0.4047 - acc: 0.9167     \n",
      "Epoch 51/100\n",
      "120/120 [==============================] - 0s - loss: 0.4012 - acc: 0.9167     \n",
      "Epoch 52/100\n",
      "120/120 [==============================] - 0s - loss: 0.3937 - acc: 0.9000     \n",
      "Epoch 53/100\n",
      "120/120 [==============================] - 0s - loss: 0.3927 - acc: 0.9333     \n",
      "Epoch 54/100\n",
      "120/120 [==============================] - 0s - loss: 0.3843 - acc: 0.9417     \n",
      "Epoch 55/100\n",
      "120/120 [==============================] - 0s - loss: 0.3870 - acc: 0.8583     \n",
      "Epoch 56/100\n",
      "120/120 [==============================] - 0s - loss: 0.3781 - acc: 0.9417     \n",
      "Epoch 57/100\n",
      "120/120 [==============================] - 0s - loss: 0.3721 - acc: 0.9417     \n",
      "Epoch 58/100\n",
      "120/120 [==============================] - 0s - loss: 0.3694 - acc: 0.9083     \n",
      "Epoch 59/100\n",
      "120/120 [==============================] - 0s - loss: 0.3631 - acc: 0.9250     \n",
      "Epoch 60/100\n",
      "120/120 [==============================] - 0s - loss: 0.3661 - acc: 0.9417     \n",
      "Epoch 61/100\n",
      "120/120 [==============================] - 0s - loss: 0.3521 - acc: 0.9583     \n",
      "Epoch 62/100\n",
      "120/120 [==============================] - 0s - loss: 0.3498 - acc: 0.9167     \n",
      "Epoch 63/100\n",
      "120/120 [==============================] - 0s - loss: 0.3459 - acc: 0.9417     \n",
      "Epoch 64/100\n",
      "120/120 [==============================] - 0s - loss: 0.3426 - acc: 0.9250     \n",
      "Epoch 65/100\n",
      "120/120 [==============================] - 0s - loss: 0.3372 - acc: 0.9500     \n",
      "Epoch 66/100\n",
      "120/120 [==============================] - 0s - loss: 0.3334 - acc: 0.9583     \n",
      "Epoch 67/100\n",
      "120/120 [==============================] - 0s - loss: 0.3288 - acc: 0.9417     \n",
      "Epoch 68/100\n",
      "120/120 [==============================] - 0s - loss: 0.3253 - acc: 0.9417     \n",
      "Epoch 69/100\n",
      "120/120 [==============================] - 0s - loss: 0.3219 - acc: 0.9667     \n",
      "Epoch 70/100\n",
      "120/120 [==============================] - 0s - loss: 0.3199 - acc: 0.9583     \n",
      "Epoch 71/100\n",
      "120/120 [==============================] - 0s - loss: 0.3157 - acc: 0.9250     \n",
      "Epoch 72/100\n",
      "120/120 [==============================] - 0s - loss: 0.3111 - acc: 0.9500     \n",
      "Epoch 73/100\n",
      "120/120 [==============================] - 0s - loss: 0.3067 - acc: 0.9667     \n",
      "Epoch 74/100\n",
      "120/120 [==============================] - 0s - loss: 0.3022 - acc: 0.9500     \n",
      "Epoch 75/100\n",
      "120/120 [==============================] - 0s - loss: 0.2993 - acc: 0.9667     \n",
      "Epoch 76/100\n",
      "120/120 [==============================] - 0s - loss: 0.2938 - acc: 0.9583     \n",
      "Epoch 77/100\n",
      "120/120 [==============================] - 0s - loss: 0.2934 - acc: 0.9667     \n",
      "Epoch 78/100\n",
      "120/120 [==============================] - 0s - loss: 0.2853 - acc: 0.9583     \n",
      "Epoch 79/100\n",
      "120/120 [==============================] - 0s - loss: 0.2866 - acc: 0.9500     \n",
      "Epoch 80/100\n",
      "120/120 [==============================] - 0s - loss: 0.2843 - acc: 0.9500     \n",
      "Epoch 81/100\n",
      "120/120 [==============================] - 0s - loss: 0.2839 - acc: 0.9667     \n",
      "Epoch 82/100\n",
      "120/120 [==============================] - 0s - loss: 0.2698 - acc: 0.9667     \n",
      "Epoch 83/100\n",
      "120/120 [==============================] - 0s - loss: 0.2737 - acc: 0.9250     \n",
      "Epoch 84/100\n",
      "120/120 [==============================] - 0s - loss: 0.2750 - acc: 0.9667     \n",
      "Epoch 85/100\n",
      "120/120 [==============================] - 0s - loss: 0.2639 - acc: 0.9750     \n",
      "Epoch 86/100\n",
      "120/120 [==============================] - 0s - loss: 0.2576 - acc: 0.9667     \n",
      "Epoch 87/100\n",
      "120/120 [==============================] - 0s - loss: 0.2534 - acc: 0.9750     \n",
      "Epoch 88/100\n",
      "120/120 [==============================] - 0s - loss: 0.2502 - acc: 0.9667     \n",
      "Epoch 89/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s - loss: 0.2481 - acc: 0.9750     \n",
      "Epoch 90/100\n",
      "120/120 [==============================] - 0s - loss: 0.2435 - acc: 0.9750     \n",
      "Epoch 91/100\n",
      "120/120 [==============================] - 0s - loss: 0.2416 - acc: 0.9750     \n",
      "Epoch 92/100\n",
      "120/120 [==============================] - 0s - loss: 0.2413 - acc: 0.9667     \n",
      "Epoch 93/100\n",
      "120/120 [==============================] - 0s - loss: 0.2331 - acc: 0.9750     \n",
      "Epoch 94/100\n",
      "120/120 [==============================] - 0s - loss: 0.2311 - acc: 0.9750     \n",
      "Epoch 95/100\n",
      "120/120 [==============================] - 0s - loss: 0.2318 - acc: 0.9583     \n",
      "Epoch 96/100\n",
      "120/120 [==============================] - 0s - loss: 0.2306 - acc: 0.9667     \n",
      "Epoch 97/100\n",
      "120/120 [==============================] - 0s - loss: 0.2217 - acc: 0.9750     \n",
      "Epoch 98/100\n",
      "120/120 [==============================] - 0s - loss: 0.2255 - acc: 0.9750     \n",
      "Epoch 99/100\n",
      "120/120 [==============================] - 0s - loss: 0.2156 - acc: 0.9750     \n",
      "Epoch 100/100\n",
      "120/120 [==============================] - 0s - loss: 0.2134 - acc: 0.9750     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1225a7160>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Model\n",
    "\n",
    "We can now run the model on our test data and check the accuracy of the model. Since our `y_test` will be a matrix with a 1 at an element and our predicted output will also be a matrix but with probabilities at each index, we can check if we got the right class by selecting the index with the highest value. In the case of `y_test`, it will always be the correct class and in the case of our prediction, it will be the class we are most confident it is as the probabilities in the output vector sum up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.966666666667\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_test_class = np.argmax(y_test,axis=1)\n",
    "y_pred_class = np.argmax(y_pred,axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_test_class, y_pred_class)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "As you can see, we achieve an incredible accuracy of 96.7% on the prediction task. This was also trained super quickly and with a minimal amount of code. We can see the power of neural networks for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Report\n",
    "\n",
    "Here are the scorse computed based on what we talked about in class.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00         6\n",
      "          1       1.00      0.91      0.95        11\n",
      "          2       0.93      1.00      0.96        13\n",
      "\n",
      "avg / total       0.97      0.97      0.97        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_class,y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A visualization of the confusion matrix for our model. The confusion matrix helps us learn what the model predicted in comparison to the ground truth class. This allows us to identify where the model goes wrong in general. You can learn more about confusion matrices [here](http://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x122aead30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cnf_matrix = confusion_matrix(y_test_class,y_pred_class)\n",
    "np.set_printoptions(precision=2)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=['Iris Setosa', 'Iris Versicolor', 'Iris Virginica'], title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
